### 备份

Kafka为每个主题分区在可配置数量（可以为每个主题配置不同备份因子）的服务器上备份日志。当集群中的服务器故障时可以自动故障转移到这些备份上，所以故障时这些消息依然可用。

Kafka默认是使用备份的——事实上用备份因子为1的主题实现了不备份的主题。

备份单元式主题分区。在未故障的情况下，Kafka中的每个分区有一个leader和0个或多个followers。备份的总数包括leader。所有的读和写操作都指向分区的leader。一般，分区比代理多，leaders均匀地在代理上分布。followers上的日志与leader上的日志相同——都有相同的偏移和相同顺序的消息（但是，当然在某个时间，leader日志的末尾可能有没有备份的消息）。

followers与一般的Kafka一样从leader消费消息，并把消息应用到自己的日志。让followers从leader拉消息有一个好处，让follower自然地将它们要应用到它们的日志中的日志条目聚集。

与大多数自动处理故障的分布式系统一样，需要有一个节点“存活”的精确定义。Kafka节点存活的两个条件：

1. 节点必须可以维持它与ZooKeeper之间的session（通过ZooKeeper心跳机制）。
2. 如果它是一个follower它必须备份发生在leader上的写操作，并且不能落下太远。

将满足这两个条件的节点称为“在同步”（“in sync”），以避免“alive”或“failed”的模糊性。leader持续追踪“在同步”的节点的集合。如果follower死掉，卡住或落后，leader会将它从在同步的备份中移除。卡住或落后备份的确定由`replica.lag.time.max.ms`配置控制。

在分布式系统术语中，Kafka只尝试处理“故障/恢复”的故障模型，即节点忽然停止工作然后恢复（可能不知道节点死掉过）。Kafka不处理节点产生任意或者恶意响应的故障。

现在可以更加精确地定义消息被committed，即当所有对应分区的in sync备份把消息加入了它们的日志。只有提交的消息才会被给到消费者。这意味着如果leader故障消费者不必担心潜在消息丢失。另一方面，生产者根据对延时和持久性的权衡倾向，可以选择是否等待消息被提交。这种倾向由生产者使用的确认设置控制。注意，主题有一个“最少数量的”in-sync备份的设置，当生产者请求一条消息被写到所有的in-sync备份的确认时，会检查这个设置。如果生产者请求不严格的确认，即使in-sync备份比最小值（可以低至只有leader）小，消息也可以被committed，被消费。

Kafka提供的保证是committed的消息不会丢失，任何时间，只要有一个in-sync备份活跃。

节点故障时，在经历过短暂的故障转移期后，Kafka仍然可用，但是网络分区可能不可用。

#### 1、备份日志：Quorums，ISRs，State Machines

Kafka分区本质上是一个备份的日志。在分布式数据系统中备份的日志是最基本的组件之一，有许多方法可以实现日志备份。备份的日志可以被其它系统以状态机器的风格（state-machine style）用作一个基本组件来实现其他的分布式系统。

备份的日志对一系列值的顺序（一般给日志条目的序号是0、1、2、……）达成一致的古城进行建模。有许多实现的方式，但是最简单快捷的方式是用leader，让leader选择提供给它的值的顺序。只要leader活跃，所有的followers只用复制值和leader选择的顺序。

其实，如果leaders不发生故障，是用不到followers的。当leader故障，会在followers中选择一个新的leader。但是，followers本身可能状态落后，或者故障，所以要选一个状态最新的的follower。日志备份算法必须提供的基础保证是，如果告诉客户端一条消息被提交了，并且leader故障，选择的新的leader必须也有这条消息。这就需要权衡：如果leader在声明消息被提交之前等待更多的followers确认这条消息，那么就会有更多的潜在的可供选择的leaders。

应对这个权衡的一般方法是提交决定和leader选择都使用**大多数表决**（majority vote）。这不是Kafka采用的方法，但可以通过这种方法来理解这个权衡。假如有2*f*+1个备份，如果在leader声明消息提交之前必须有*f*+1个备份接收到了这条消息，并且通过从至少*f*+1个备份中选择具有最完整日志的的follower作为leader，那么，不超过*f*个备份故障时，可以保证leader具有所有提交过的消息。因为在任意*f*+1个备份中，肯定至少有一个备份包含了所有的提交的消息。这种方式有一个优点：延迟由最快的那些服务器决定。即，如果备份因子是3，延迟由最快的那个follower决定，而不是由比较慢的那个决定。

大多数表决的缺点是，它无法容忍许多备份故障，可能会导致没有可选的的leaders。要容忍一个备份故障需要有3个数据拷贝，要容忍两个备份故障需要5个数据拷贝。这需要太多的硬盘空间，对于大数据量的场合不实际。这可能是**quorum算法**在ZooKeeper这样的共享集群配置中更加常见，而在主要目的是数据存储的系统中不常见的原因。比如，在HDFS中，namenode的高可用特性是基于多数表决构建的，但是这对于数据本身是高开销的方式，所以没有采用。

Kafka采用一种稍微不同的方法来选择它的quorum集。Kafka动态地维护一个in-sync备份（ISR），ISR是与leader状态相同的。只有这个quorum集中的成员才有资格成为leader。对于Kafka的写操作，只有在所有的ISR都收到写操作后才会被认为是已经提交的。当ISR集发生变化时，它会被持久化到ZooKeeper。因此，ISR集合中的任何备份都是有成为leader资格的。这是Kafka使用模型的一个重要特性，Kafka有许多的分区，确保leadership平衡很重要。使用这种ISR模型和*f*+1个备份，Kafka主题可以容忍*f*个备份故障，而不丢失提交的消息。

对于Kafka的大多数应用场景，这种权衡是合理的。在实践中，为了容忍*f*个备份故障，多数表决和ISR方式会在提交消息之前等待相同数量的备份确认（比如，为了应对一个备份故障，一个多数的quorum需要3个备份和一个确认，ISR方式需要两个备份和一个确认）。多数表决的一个优势是，不用等待最慢的服务器完成提交。但是，通过让客户端选择是否阻塞消息提交，是可以改善的，并且低的备份因子需求带来的额外的吞吐量和硬盘空间使得采用ISR方式是值得的。

另一个重要的设计是，Kafka不需要故障节点恢复时节点的数据是完整的。对于备份算法来说，在故障恢复时，要求保持数据完整是很常见的。但是有两个问题，其一，在持久化数据系统的操作中硬盘错误是最常见的问题，这经常会导致数据不完整；其二，即使这不是问题，也不希望为了一致性的保证每次写操作都要进行*fsync*（强制同步）操作，这会将性能降低两个或三个数量级。Kafka的协议允许备份重新加入ISR，即使在故障时丢失了数据，只要加入前保证它与leader是完全同步的。

#### 2、unclean leader选择：所有备份都故障的情况

Kafka关于数据丢失的保证是基于至少一个备份是in-sync的。如果所有备份分区的节点都故障，这种保证不再有用。

但是，实用的系统需要在所有备份故障时采取一些合理的操作。可以采取的两种行为是：

1. 等待一个ISR中的备份恢复，并且选择这个备份作为leader（期望它保留了完整的数据）。
2. 选择第一个恢复的备份（不一定是ISR成员）作为leader。

这是可用性和一致性之间的权衡。等待ISR中的备份的话，只要这些备份没有恢复，系统就不可用。如果这些备份被摧毁或者数据丢失，系统将一致无法恢复。另一方面，如果一个非in-sync的备份恢复，并允许它成为leader，它的日志会成为标准（truth），即使它并不拥有完整的提交消息。从0.11.0.0开始，Kafka默认使用第一种策略，并等待一个一致的备份。通过`unclean.leader.election.enable`配置可以改变这种行为，以支持更倾向于短恢复时间的应用场景。

这种困境不只Kafka会遇到，所有基于quorum模式的系统都有。例如，在多数表决模式中，如果多数的服务器故障且无法恢复，那么就必须选择丢失全部数据，还是违背一致性把仅仅留存在活跃服务器上的数据作为标准（truth）。

#### 3、可用性和持久性保证

当写到Kafka时，生产者可以选择等待消息被备份0，1或者所有（-1）的备份确认。注意，”被所有的备份确认“不保证所有的备份都收到了这条消息。默认情况下，当acks=all时，只要当前所有的in-sync备份收到了这条消息就会发送确认。例如，如果主题被配置为只有两个备份，并且一个故障（即，只剩下一个in-sync的备份），那么，指定acks=all的写操作会成功。可是，如果剩下的备份也发生故障，这些写操作可能会丢失。尽管这可以保证最大的分区可用性，这种行为对于更看重持久性的用户来说是不希望看到的。因而，Kafka提供了两个主题级别的配置，可以用于更看重消息持久性的场合：

1. 禁用unclean leader选择——如果所有的备份都不可用，分区会在最近的leader恢复之前保持不可用的状态。选择保持不可用，而不是冒数据丢失的风险。
2. 指定一个最小的ISR大小（ISR备份的数量大小）——分区只有在ISR的大小超过一定的最小值时才接受写操作，以防止消息只写到一个备份，而这个备份故障从而导致的消息丢失。这个设置只有在生产者使用acks=all的时候生效，并且保证消息会被至少这个数量的in-sync备份确认。这个设置提供了一个一致性和可用性之间的权衡。更大的值保证了更好的一致性，因为消息会被保证写到了更多的备份，降低了消息丢失的概率。但是，降低了可用性，因为如果in-sync的备份低于最小阈值分区将不能进行写操作。

#### 4、备份管理

以上关于备份日志的讨论只是针对一个日志的，即，一个话题分区。但是Kafka集群要管理成百上千这样的分区。Kafka尝试用轮循的方式来平衡集群内的分区，以避免把高体量的主题的所有分区集聚到少数的节点上。同样，尝试平衡leadership以便每个节点是一部分它的分区的leader。

优化leader选择过程也是很重要的，它是不可用性的关键窗口。当节点故障时，原始的leader选择实现会停止为这个节点中的所有分区运行每个分区一个的leader选择。而Kafka选择一个代理作为”controller“。这个controller在代理级别侦测故障，并负责为故障代理影响的分区改变leader。结果是，Kafka可以批次地处理leadership的变更通知，这使得对于大量分区的leader选择处理更容易更快。如果controller故障，存活代理中的一个会成为新的contoller。

