### 4.10、反序列化器

生产者需要用序列化器把对象转换成字节数组再发送给 Kafka。类似地，消费者需要用反序列化器把从 Kafka 接收到的字节数组转换成 Java 对象。生成消息使用的序列化器与读取消息使用的反序列化器应该是一一对应的。

对于开发者来说，必须知道写入主题的消息使用的是哪一种序列化器，并确保每个主题里只包含能够被反序列化器解析的数据。使用 Avro 和 schema 注册表进行序列化和反序列化的优势在于：`AvroSerializer` 可以保证写入主题的数据与主题的 schema 是兼容的，也就是说，可以使用相应的反序列化器和 schema 来反序列化数据。另外，在生产者或消费者里出现的任何一个与兼容性有关的错误都会被捕捉到，它们都带有消息描述，也就是说，在出现序列化错误时，就没必要再去调试字节数组了。

尽管不建议使用自定义的反序列化器，还是会简单地演示如何自定义反序列化器，然后再举例演示如何使用 Avro 来反序列化消息的键和值。

#### 4.10.1、自定义反序列化器

有自定义对象如下：

```java
public class Customer {
        private int customerID;
        private String customerName;
　
        public Customer(int ID, String name) {
                this.customerID = ID;
                this.customerName = name;
        }
　
  public int getID() {
    return customerID;
  }
　
  public String getName() {
    return customerName;
  }
}
```

自定义反序列化器如下：

```java
import org.apache.kafka.common.errors.SerializationException;
import java.nio.ByteBuffer;
import java.util.Map;

public class CustomerDeserializer implements Deserializer<Customer> {// ➊
    @Override
    public void configure(Map configs, boolean isKey) {
        // 不需要做任何配置
    }

    @Override
    public Customer deserialize(String topic, byte[] data) {

        int id;
        int nameSize;
        String name;

        try {
            if (data == null)
            	return null;
            if (data.length < 8)
            	throw new SerializationException("Size of data received by IntegerDeserializer is shorter than expected");

            ByteBuffer buffer = ByteBuffer.wrap(data);
            id = buffer.getInt();
            nameSize = buffer.getInt();

            byte[] nameBytes = new byte[nameSize];
            buffer.get(nameBytes);
            name = new String(nameBytes, "UTF-8");

            return new Customer(id, name);// ➋

        } catch (Exception e) {
            throw new SerializationException("Error when serializing Customer to byte[] "
            + e);
        }
    }

    @Override
    public void close() {
        // 不需要关闭任何东西
    }
}
```

❶ 消费者也需要使用 `Customer` 类，这个类和序列化器在生产者和消费者应用程序里要相互匹配。在一个大型的企业里，会有很多消费者和生产者共享这些数据，这对于企业来说算是一个挑战。

❷ 把序列化器的逻辑反过来，从字节数组里获取 customer ID 和 name，再用它们构建需要的对象。

使用反序列化器的消费者代码：

```java
    Properties props = new Properties();
    props.put("bootstrap.servers", "broker1:9092,broker2:9092");
    props.put("group.id", "CountryCounter");
    props.put("key.deserializer",
            "org.apache.kafka.common.serialization.StringDeserializer");
    props.put("value.deserializer",
            "org.apache.kafka.common.serialization.CustomerDeserializer");

    KafkaConsumer<String, Customer> consumer = new KafkaConsumer<>(props);

    consumer.subscribe(Collections.singletonList("customerCountries"))
    while (true) {
        ConsumerRecords<String, Customer> records = consumer.poll(100);
        for (ConsumerRecord<String, Customer> record : records) {
            System.out.println("current customer Id: " + record.value().getID() +
                      " and current customer name: " + record.value().getName());
        }
    }
```

再强调一次，并不建议使用自定义序列化器和自定义反序列化器。它们把生产者和消费者紧紧地耦合在一起，并且很脆弱，容易出错。建议使用标准的消息格式，比如 JSON、Thrift、Protobuf 或 Avro。

#### 4.10.2、在消费者中进行 Avro 反序列化

读取`Customer`类的对象的消费者应用程序示例：

```java
    Properties props = new Properties();
    props.put("bootstrap.servers", "broker1:9092,broker2:9092");
    props.put("group.id", "CountryCounter");
    props.put("key.serializer",
            "org.apache.kafka.common.serialization.StringDeserializer");
    props.put("value.serializer",
            "io.confluent.kafka.serializers.KafkaAvroDeserializer");// ➊
    props.put("schema.registry.url", schemaUrl);// ➋
    String topic = "customerContacts";

    KafkaConsumer consumer = new KafkaConsumer(createConsumerConfig(brokers,groupId, url));
    consumer.subscribe(Collections.singletonList(topic));

    System.out.println("Reading topic:" + topic);

    while (true) {
        ConsumerRecords<String, Customer> records = consumer.poll(1000);// ➌
    
        for (ConsumerRecord<String, Customer> record: records) {
            System.out.println("Current customer name is: " +
                    record.value().getName());// ➍
        }
        consumer.commitSync();
    }
```

❶ 使用 `KafkaAvroDeserializer` 来反序列化 Avro 消息。

❷ `schema.registry.url` 是一个新的参数，它指向 schema 的存放位置。消费者可以使用由生产者注册的 schema 来反序列化消息。

❸ 将生成的类 `Customer` 作为值的类型。

❹ `record.value()` 返回的是一个 `Customer` 实例，接下来就可以使用它了

### 4.11、独立消费者——为什么以及怎样使用没有群组的消费者

使用消费者群组带来很多便利，但是，有时可能只需要一个消费者从一个主题的所有分区或者某个特定的分区读取数据。这个时候就不需要消费者群组和再均衡了，只需要把主题或者分区分配给消费者，然后开始读取消息并提交偏移量。

如果是这样的话，就不需要订阅主题，取而代之的是为自己分配分区。一个消费者可以订阅主题（并加入消费者群组），或者为自己分配分区，但不能同时做这两件事情。

一个消费者是为自己分配分区并从分区里读取消息的示例：

```java
    List<PartitionInfo> partitionInfos = null;
    partitionInfos = consumer.partitionsFor("topic");// ➊

    if (partitionInfos != null) {
        for (PartitionInfo partition : partitionInfos)
            partitions.add(new TopicPartition(partition.topic(), partition.partition()));
        consumer.assign(partitions);// ➋

        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(1000);

            for (ConsumerRecord<String, String> record: records) {
                System.out.println("topic = %s, partition = %s, offset = %d, customer = %s, country = %s\n",
                        record.topic(), record.partition(), record.offset(),
                        record.key(), record.value());
            }
            consumer.commitSync();
        }
    }
```

❶ 向集群请求主题可用的分区。如果只打算读取特定分区，可以跳过这一步。

❷ 知道需要哪些分区之后，调用 `assign()` 方法。

除了不会发生再均衡，也不需要手动查找分区，其他的看起来一切正常。但是，如果主题增加了新的分区，消费者并不会收到通知。所以，要么周期性地调用 `consumer.partitionsFor()` 方法来检查是否有新分区加入，要么在添加新分区后重启应用程序。

### 4.12、旧版的消费者 API

本章使用的 Java `KafkaConsumer` 客户端是 `org.apache.kafka.clients` 包的一部分。Kafka 还有两个旧版本的 Scala 消费者客户端，它们是 `kafka.consumer` 包的一部分，属于 Kafka 核心模块。它们分别被叫作 `SimpleConsumer`（简单消费者，实际上也不是那么简单，它们是对 Kafka API 的轻度包装，可以用于从特定的分区和偏移量开始读取消息）和高级消费者。高级消费者指的就是 `ZookeeperConsumerConnector`，它有点像现在的消费者，有消费者群组，有分区再均衡，不过它使用 Zookeeper 来管理消费者群组，并不具备提交偏移量和再均衡的可操控性。

因为现在的消费者同时支持以上两种行为，并且为开发人员提供了更高的可靠性和可操控性，所以不再讨论旧版 API。
