#### **事件流是什么**？

技术上讲，事件流（event streaming）是指以事件的流（streams of events）的形式从事件源（比如数据库，传感器，移动设备，云服务，软件应用）实时捕获数据的事件；持久地将这些事件的流保存以用于后续的提取；也可以追溯性地（retrospectively）操作、处理、响应这些事件的流；并且根据需要发送这些事件的流到不同的目标技术。事件流因此确保了数据的连续的流动和解释（interpretation），以便正确的信息在正确的位置，在正确的时间。

#### 事件流的用途

事件流在许多企业和组织中有广泛地应用场景。它的例子包括：

* 实时处理付款和金融业务，比如在股票交易，银行和保险中；
* 实时追踪和监控汽车、卡车、船队和航运，比如在物流和汽车行业；
* 持续地获取和分析来自IoT设备或其他设施的传感器数据，比如在工厂和风电场；
* 收集并立即响应客户的交互和订单，比如在零售、宾馆和旅行行业和移动应用中；
* 监控病人的医院护理情况并预测病情变化，以确保紧急情况及时得到治疗；
* 连接，保存，提供公司不同部门产生的数据；
* 用作数据平台，事件驱动架构和微服务的基础。

#### “Kafka是一个事件流平台”的含义

Kafka有三个关键能力，可以用一个经过实战考验（battle-tested）方案实现端到端事件流的应用场景：

1. **发布**（写）和**订阅**（读）事件的流，包括从其它系统连续的导入/导出数据。
2. 根据期望持久且可靠的**存储**事件的流。
3. 实时或追溯性的**处理**事件的流。

所有这些功能都是以分布式，高可扩展，弹性，容错，并且安全的方式提供的。Kafka可以部署在裸机（bare-metal）硬件，虚拟机，和容器，并且即可以是内部部署（on-premise）也可以在云上。可以在自己管理的Kafka环境和不同供应商提供的完全托管的服务之间进行选择。

#### Kafka运行简介

Kafka是一个由通过高性能TCP网络协议沟通的服务器和客户端构成的分布式系统。

**服务器**：Kafka运行为一个或者多个服务器构成的集群，这些服务器可以分布在多个数据中心或云区域。某些服务构成存储曾，叫作**brokers**（代理）。其它服务器运行**Kafka Connect**来持续地导入和导出数据为事件流，以将Kafka和已经存在的系统（比如关系型数据库或者其它Kafka集群）进行集成。Kafka可以用于实现关键任务的（mission-critical）使用场景，因为Kafka集群是**高度可扩展和容错**的：如果某些服务器故障，其它服务器会接管它们的工作以确保在没有任何数据丢失的情况下持续的进行操作。

**客户端**：让用户可以用来编写即使在网络问题或者机器故障的情况下仍然可以并行、可扩展、容错的读、写、处理事件的流的分布式应用和微服务。Kafka带有一些Kafka社区提供的拥有数十个（dozens of）客户端的增强型客户端，Java和Scala客户端包含高级别的Kafka Streams库，有Go，Python，C/C++和其它编程语言的客户端，也有REST APIs。

#### 主要概念和术语

**事件（Event）**世界上或者你的记录的业务中“发生了某些事情”的实事。在文档中也被叫作_记录（record）_或者_消息（message）_。当从Kafka读写数据的时候，是以事件的形式进行的。从概念上讲，事件有一个key、value、时间戳，和可选的元数据头（metadata headers）。比如：

* 事件key：“Alice”
* 事件value：“Made a payment of $200 to Bob”
* 事件时间戳：“Jun 25, 2020 at 2:06 p.m.”

**生产者（Producers）**是往Kafka发布（写）事件的客户端应用，**消费者（consumers）**是订阅（读和处理）这些事件的客户端应用。在Kafka中，生产者和消费者是完全解耦的并且相互不可知的（agnostic），这是使Kafka获得高可扩展性的关键设计元素。这样，生产者从不需要等待消费者。Kafka提供了多种保证，比如处理事件exactly-once的能力。

事件被组织并持久地存储在**主题（topics）**中。主题类似于文件系统中的文件夹，事件则是文件夹中的文件。例如，主题的名字可以是“payments”。Kafka中的主题总是多生产者和多订阅者的：主题可以有0个、1个或者多个生产者向它写入事件，也可以有0个、1个或者多个消费者订阅这些事件。可以根据需要读取主题中的事件——与传统的消息系统不同，事件被消费后不会被删除。作为替代，可以通过一个针对每个主题的配置设置定义Kafka保留事件的时长，超过这个时长的事件会被摈弃。Kafka的性能相对于数据大小事实上是恒定的，因此长时间保存数据是完全没问题的。

主题是**分区（partitioned）**的，这意味着主题分布在不同Kafka代理上的几个“桶”中。这种数据的分布式存放对于可扩展性是非常重要的，因为这能让客户端应用同时对多个代理进行读写。当新的事件发布到主题时，它事实上被追加到主题的一个分区上。具有相同事件key（比如，一个消费者或者汽车ID）的事件会被写到相同的分区，并且Kafka保证特定主题分区的任何消费者会按照事件写入的顺序读取那个分区的事件。

![img](/assets/streams-and-tables-p1_p4.png)

###### 图片：这个例子主题具有四个分区P1-P4。两个相互独立的生产者客户端发布事件，通过网络把事件写到主题的分区。具有相同key（在图中用颜色表示）的事件被写到相同的分区。注意，如果合适两个生产者都可以往相同的分区写入事件。

为了确保数据的容错性和高可用，每个主题都可以进行**备份**，甚至可以跨越地理区域或者数据中心备份，以便总是有多个代理保留了数据的一个副本，以防意外发生或者想要对代理进行维护等情况。常见的生产环境设置是备份因子为3，即，数据总会保持3个副本。备份是在主题分区的级别进行的。

#### Kafka APIs

除了用于管理任务的命令行工具，对于Java和Scala，Kafka还有5种核心APIs：

* Admin API：管理和检测主题、代理、和其它Kafka对象。
* Producer API：把事件的流发布（写）到一个或者多个Kafka主题。
* Consumer API：订阅（读）一个或者多个主题，并且处理为它们而生产的事件的流。
* Kafka Streams API：实现流处理应用和微服务。它提供了高级别的函数来处理事件流，包括转换，以及有状态的操作和聚合、连接、windowing，基于事件时间的处理，等等。输入按照顺序从一个或者多个主题读取，生成输出到一个或者多个主题，有效地将输入流转换为输出流。
* Kafka Connect API：构建和运行可重用的数据输入/输出连接器，消费（读）或者生产（写）事件流自（到）外部系统和应用，以将外部系统和应用与Kafka集成。例如，一个对关系型数据库（比如PostgreSQL）的连接器可以获取一组表的每个变化。但是，在实践中，一般不需要自己实现连接器，因为Kafka社区已经提供了数百种可用的连接器。



